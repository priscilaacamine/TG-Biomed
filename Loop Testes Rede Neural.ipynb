{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_size, layers_size[0])])\n",
    "        for i in range(0, self.num_layers-2):\n",
    "            self.linears.extend([nn.Linear(layers_size[i], layers_size[i+1])])              \n",
    "        self.linears.append(nn.Linear(layers_size[-1], output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[0:-1]:                   \n",
    "            x = torch.tanh(layer(x))\n",
    "        x = self.linears[-1](x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonRepeatedRandomInt(low, upper, N):\n",
    "        import numpy as np\n",
    "        import random\n",
    "        \n",
    "        numbers = np.arange(low, upper, 1)\n",
    "        random.shuffle(numbers)\n",
    "        shuffleNumbers = np.array(numbers)[0:int(N)]\n",
    "        shuffleNumbers = np.sort(shuffleNumbers)\n",
    "        \n",
    "        return shuffleNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(file, idx, net_param, ncamadas, n_features, features_col, features_name, random_state, data_train, data_train_resample, test_loss, test_cos_sim, net):\n",
    "    import csv\n",
    "    \n",
    "    \n",
    "    headerCSVList= []\n",
    "   \n",
    "    headerCSVList.append('index')\n",
    "    headerCSVList.append('net_param')\n",
    "    headerCSVList.append('ncamadas')\n",
    "    headerCSVList.append('n_features')\n",
    "    headerCSVList.append('features_col')\n",
    "    headerCSVList.append('features_name')\n",
    "    headerCSVList.append('random_state')\n",
    "    headerCSVList.append('data_train')\n",
    "    headerCSVList.append('data_train_resample')\n",
    "    headerCSVList.append('test_loss')\n",
    "    headerCSVList.append('test_cos_sim')\n",
    "    headerCSVList.append('net')\n",
    "    \n",
    "    #---printing file results---------\n",
    "    rowCSVList=[]   \n",
    "    rowCSVList.append(idx)\n",
    "    rowCSVList.append(net_param)\n",
    "    rowCSVList.append(ncamadas)\n",
    "    rowCSVList.append(n_features)\n",
    "    rowCSVList.append(features_col)\n",
    "    rowCSVList.append(features_name)\n",
    "    rowCSVList.append(random_state)\n",
    "    rowCSVList.append(data_train)\n",
    "    rowCSVList.append(data_train_resample)\n",
    "    rowCSVList.append(test_loss)\n",
    "    rowCSVList.append(test_cos_sim)\n",
    "    rowCSVList.append(net)\n",
    "    \n",
    "   \n",
    "    with open(file, 'a') as csvfileWrite:\n",
    "        spamwriter = csv.writer(csvfileWrite, delimiter=',')\n",
    "        if idx==50000:\n",
    "            spamwriter.writerow(headerCSVList)\n",
    "           \n",
    "        spamwriter.writerow(rowCSVList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for teste in range(66601, 100000):\n",
    "    data = pd.read_pickle(\"data_closed_rigid_o\")\n",
    "    pf50 = data[\"PF50_closed_rigid\"].values\n",
    "\n",
    "    # Sortear quais caracteristicas usar\n",
    "    n_features = np.random.randint(low=1, high=26)\n",
    "    #print('Numero de features: %d' % (n_features))\n",
    "    columns = nonRepeatedRandomInt(low=0, upper=183, N=n_features)\n",
    "    #print(columns)\n",
    "\n",
    "    features = data.iloc[:,columns]\n",
    "    features_names = features.columns\n",
    "    X = features.values\n",
    "    y = pf50.reshape(len(pf50), 1)\n",
    "\n",
    "    # Separando o dataset em treino e teste\n",
    "    random_state =  np.random.randint(low=1, high=100)\n",
    "    #print('Random State: %d' % (random_state))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "    # Histograma dos dados de treino\n",
    "    for nbins in range(10,1,-1):\n",
    "        n, bins, patches = plt.hist(y_train, nbins)\n",
    "        if np.min(n)>3:\n",
    "            break\n",
    "\n",
    "    # Indica qual o bin correspondente para cada dado\n",
    "    indices = np.digitize(y_train, bins)\n",
    "\n",
    "    # Arrumar o caso do último valor que não é incluído no último bin\n",
    "    indices[indices>nbins]=nbins\n",
    "    indices = indices.reshape(len(indices),)\n",
    "\n",
    "    # SMOTE - Data Augmentation\n",
    "    data_train = np.hstack([X_train, y_train])\n",
    "    sm = SMOTE(k_neighbors=np.int(n.min()-1), random_state=42)\n",
    "    data_train_res, indices_res = sm.fit_resample(data_train, indices)\n",
    "\n",
    "    # Histograma dos dados de treino resample\n",
    "    nbins = 5\n",
    "    n_res, bins_res, patches_res = plt.hist(data_train_res[:,-1], nbins)\n",
    "\n",
    "    # Declarando o dado resample como dado de treino\n",
    "    X_train = data_train_res[:,0:-1]\n",
    "    y_train = data_train_res[:,-1].reshape(data_train_res.shape[0],1)\n",
    "\n",
    "    # Ajustando o escalamento ao dado de TREINAMENTO (fit) e transformando o dado de treinamento\n",
    "    scaler_x = MinMaxScaler()\n",
    "    X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    num_layers = np.random.randint(low=3, high=10)\n",
    "    #print('Number of layers: %d' % (num_layers))\n",
    "    layer_init_size = np.random.randint(low=1, high=20)\n",
    "    #print('Inicial size layer: %d' % (layer_init_size))\n",
    "    layers_size = np.linspace(layer_init_size, 1, num_layers-1, dtype=int)\n",
    "    #print(layers_size)\n",
    "\n",
    "    net = Net(input_size=X.shape[1], num_layers=num_layers, layers_size=layers_size , output_size=1)\n",
    "\n",
    "    # Choose optmizer and loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = 0.05, momentum = 0.9) # lr = learning rate\n",
    "\n",
    "    # Treinamento utilizando o dado de treino transformado\n",
    "    epochs = 5000\n",
    "    lossr = np.zeros(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        inputs = torch.autograd.Variable(torch.Tensor(X_train_scaled.astype(np.float32)).float())\n",
    "        targets = torch.autograd.Variable(torch.Tensor(y_train_scaled.astype(np.float32)).float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = net(inputs)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossr[epoch] = loss.item()\n",
    "\n",
    "        #if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "            #print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))\n",
    "\n",
    "    torch.save(net.state_dict(), 'F:\\\\Priscila Acamine\\\\TG-Biomed\\\\Nets2_50000\\\\net_'+str(teste))\n",
    "\n",
    "    epoch = np.arange(0, epochs)\n",
    "    # plt.plot(epoch,lossr)\n",
    "    # plt.show()\n",
    "\n",
    "    # Transformando o dado de teste\n",
    "    X_test_scaled = scaler_x.fit_transform(X_test)\n",
    "    y_test_scaled = scaler_y.fit_transform(y_test)\n",
    "\n",
    "    # Avaliando a acurácia do modelo utilizando os dados de teste transformados\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(X_test_scaled.astype(np.float32)).float())\n",
    "    targets = torch.autograd.Variable(torch.Tensor(y_test_scaled.astype(np.float32)).float())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = net(inputs)\n",
    "    loss = criterion(out, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    predicted = out.data\n",
    "\n",
    "#     print('Teste Loss: %.4f' % (loss.item()))\n",
    "\n",
    "    # Implementando a métrica do Cosine Similarity\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cos_sim = cos(targets, predicted)\n",
    "#     print('Teste Cosine Similarity: %.4f' % (cos_sim.item()))\n",
    "\n",
    "    ninput = np.arange(len(X_test))\n",
    "    '''plt.figure()\n",
    "    plt.plot(ninput, y_test_scaled, '-*', label = 'target')\n",
    "    plt.plot(ninput, predicted.numpy(), '-+', label = 'predicted')\n",
    "    plt.title('Dados Normalizados')\n",
    "    plt.legend()\n",
    "    plt.show()'''\n",
    "\n",
    "    # Fazendo a transformação inversa para desnormalizar o dado de teste\n",
    "    predicted_desnorm = scaler_y.inverse_transform(predicted.numpy())\n",
    "    target_desnorm = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "    '''plt.figure()\n",
    "    plt.plot(ninput, target_desnorm, '-*', label = 'target')\n",
    "    plt.plot(ninput, predicted_desnorm, '-+', label = 'predicted')\n",
    "    plt.title('Desnormalizado target_desnorm')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    '''\n",
    "    print_results(file='Loop_Testes_Regressao2.csv', idx=teste, net_param=net.parameters, ncamadas=num_layers, n_features=n_features, features_col=columns, \n",
    "                  features_name=list(features_names), random_state=random_state, data_train=data_train.shape[0], \n",
    "                  data_train_resample=data_train_res.shape[0], test_loss=loss.item(), test_cos_sim=cos_sim.item(), net='net_'+str(teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
