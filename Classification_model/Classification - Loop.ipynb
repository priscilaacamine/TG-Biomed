{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import mode\n",
    "from pickle import dump, load\n",
    "import os\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_size, layers_size[0])])\n",
    "        for i in range(0, self.num_layers-2):\n",
    "            self.linears.extend([nn.Linear(layers_size[i], layers_size[i+1])])              \n",
    "        self.linears.append(nn.Linear(layers_size[-1], output_size))\n",
    "\n",
    "# Última camada sem função de ativação --> crossentropy já aplica softmax\n",
    "# ReLU em intermediárias\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[0:-1]:                   \n",
    "            x = F.relu(layer(x))\n",
    "        x = (self.linears[-1](x))\n",
    "        return(x)\n",
    "\n",
    "# # Aplicando função de ativação na última camada tbm\n",
    "# ### Tentar mudar pra sigmoide se deixar a normalização de 0,1\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.linears:                   \n",
    "#             x = torch.sigmoid(layer(x))\n",
    "# #             x = F.relu(layer(x))\n",
    "#         return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonRepeatedRandomInt(low, upper, N):\n",
    "        import numpy as np\n",
    "        import random\n",
    "        \n",
    "        numbers = np.arange(low, upper, 1)\n",
    "        random.shuffle(numbers)\n",
    "        shuffleNumbers = np.array(numbers)[0:int(N)]\n",
    "                \n",
    "        return shuffleNumbers\n",
    "\n",
    "def createSurrogate(X):\n",
    "    Xsur  = np.zeros_like(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        Xsur[:,i] = X[nonRepeatedRandomInt(0, X.shape[0], X.shape[0]),i]\n",
    "    return Xsur\n",
    "\n",
    "def save_checkpoint(state, is_best, filename):\n",
    "    if is_best:\n",
    "        torch.save(state, filename)\n",
    "#         print('*****Saved epoch: %d *****' % (state['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Salva dado de treino/vali e dado de teste\n",
    "\n",
    "# # Read clean data (features <10% excluded) = 25 features\n",
    "# data = pd.read_pickle(\"data_closed_rigid_o_adjusted_clean\")\n",
    "# pf50 = np.round(data[\"PF50_closed_rigid\"].values, 2)\n",
    "\n",
    "# # Usar todas as 24 características + ones\n",
    "# features = data.copy()\n",
    "# features.drop(\"PF50_closed_rigid\", axis=1, inplace=True)\n",
    "\n",
    "# features_names = features.columns\n",
    "# print('Features: ',list(features_names))\n",
    "# X = features.values.astype('float')\n",
    "# y = pf50.reshape(len(pf50), 1)\n",
    "\n",
    "# # Separando o dataset em treino/vali e teste (treino/vali 70%, teste 30%)\n",
    "# X_train_vali, X_test, y_train_vali, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Divide em classes de acordo com a os quartis do dado de treino/vali\n",
    "# quart = np.quantile(y_train_vali,[0.25, 0.5, 0.75])\n",
    "# print(quart)\n",
    "# y_train_vali_class = np.digitize(y_train_vali, quart)\n",
    "# y_test_class = np.digitize(y_test, quart)\n",
    "# print((y_train_vali_class).shape)\n",
    "# print((y_test_class).shape)\n",
    "\n",
    "# # Salvando os dados em datasets separados\n",
    "\n",
    "# np.save('X_test.npy', X_test)\n",
    "# np.save('y_test.npy', y_test)\n",
    "# np.save('y_test_class.npy', y_test_class)\n",
    "# np.save('X_train_vali.npy', X_train_vali)\n",
    "# np.save('y_train_vali.npy', y_train_vali)\n",
    "# np.save('y_train_vali_class.npy', y_train_vali_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Net, random_state, num_layers, layers_size, net_param, criterion, learning_rate, optimizer, epochs, vali_best_epoch, vali_best_acc, vali_best_loss, vali_best_R, vali_sur_acc, vali_sur_loss, vali_sur_R, vali_c_matrix, vali_c_matrix_perc, test_acc, test_loss, test_R, test_predicted, test_c_matrix, test_c_matrix_perc]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Create empty xlsx with header\n",
    "# tags = [\"Net\",\"random_state\",\"num_layers\",\"layers_size\",\"net_param\",\n",
    "#         \"criterion\",\"learning_rate\",\"optimizer\",\"epochs\",\n",
    "#         \"vali_best_epoch\",\"vali_best_acc\",\"vali_best_loss\",\"vali_best_R\",\n",
    "#         \"vali_sur_acc\",\"vali_sur_loss\",\"vali_sur_R\",\"vali_c_matrix\", \n",
    "#         \"vali_c_matrix_perc\", \"test_acc\",\"test_loss\",\"test_R\",\n",
    "#         \"test_predicted\",\"test_c_matrix\",\"test_c_matrix_perc\"]\n",
    "# df_nets = pd.DataFrame(columns = tags)\n",
    "# print(df_nets)\n",
    "# df_nets.to_excel ('classification_nets_empty.xlsx', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# loop de redes\n",
    "Nnets = 5\n",
    "# Load initial xlsx\n",
    "df_nets = pd.read_excel('classification_nets_empty.xlsx', index_col=None, header=0)\n",
    "\n",
    "# Load data\n",
    "X_train_vali = np.load('X_train_vali.npy')\n",
    "y_train_vali_class = np.load('y_train_vali_class.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test_class = np.load('y_test_class.npy')\n",
    "\n",
    "if os.path.exists('Nets') == False: \n",
    "    os.makedirs('Nets')\n",
    "\n",
    "for i in range(Nnets):\n",
    "    number_str = str(i)\n",
    "    zero_filled_number = number_str.zfill(5)\n",
    "    \n",
    "    # Separando o treino da vali (treino 90% = 63% total, validação 10% = 7% total)\n",
    "    # Deixar o high variável?\n",
    "    random_state = np.random.randint(0, Nnets*10, 1)[0]\n",
    "#     print('Random State: %d' % (random_state))\n",
    "    X_train, X_vali, y_train_class, y_vali_class = train_test_split(X_train_vali, y_train_vali_class, test_size=0.1, random_state=random_state)\n",
    "\n",
    "    # Escalamento e Transformação dos dados\n",
    "    scaler_x = RobustScaler(with_centering=True)\n",
    "    X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "    X_vali_scaled = scaler_x.transform(X_vali)\n",
    "    X_test_scaled = scaler_x.transform(X_test)\n",
    "    # save the scaler\n",
    "    dump(scaler_x, open('./Nets/'+zero_filled_number+'_scaler_x.pkl', 'wb'))\n",
    "    \n",
    "    # Create Surrogate ## Para validação ou para teste?\n",
    "    X_vali_scaled_sur = createSurrogate(X_vali_scaled)\n",
    "    \n",
    "    # Parâmetros da rede\n",
    "    torch.manual_seed(1234)\n",
    "    num_layers = 4\n",
    "#     print('Number of layers: %d' % (num_layers))\n",
    "    layer_init_size = np.random.randint(low=1, high=24)\n",
    "#     print('Inicial size layer: %d' % (layer_init_size))\n",
    "    layer_2_size = np.random.randint(low=1, high=24)\n",
    "    layers_size = [layer_init_size, layer_2_size, 4]\n",
    "#     print('Layers sizes:', list(layers_size))\n",
    "    net = Net(input_size=X_train.shape[1], num_layers=num_layers, layers_size=layers_size , output_size=4)\n",
    "    \n",
    "    # Choose optmizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = np.random.randint(low=1, high=100)/1000\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "    \n",
    "    # Treinamento \n",
    "    epochs = 50000\n",
    "    loss_train = np.zeros(epochs)\n",
    "    loss_vali = np.zeros(epochs)\n",
    "    acc_vali = np.zeros(epochs)\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        inputs = torch.autograd.Variable(torch.Tensor(X_train_scaled.astype(np.float32)).float())\n",
    "        targets = torch.autograd.Variable(torch.Tensor(y_train_class).long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = net(inputs)\n",
    "        loss = criterion(out, targets.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "\n",
    "        # Validação\n",
    "        if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "            inputs_vali = torch.autograd.Variable(torch.Tensor(X_vali_scaled.astype(np.float32)).float())\n",
    "            targets_vali = torch.autograd.Variable(torch.Tensor(y_vali_class).long())\n",
    "            out_vali = net(inputs_vali)\n",
    "            loss_v = criterion(out_vali, targets_vali.squeeze())\n",
    "            loss_vali[epoch] = loss_v.item()\n",
    "            _, predicted = torch.max(out_vali.data, 1)\n",
    "\n",
    "            # Calcula acurácia\n",
    "            error_count = y_vali_class.size - np.count_nonzero((targets_vali.squeeze() == predicted) .numpy())\n",
    "            acc_vali[epoch] = 100 * torch.sum(targets_vali.squeeze() == predicted) / y_vali_class.size\n",
    "            \n",
    "            r_vali = np.corrcoef(predicted.detach().numpy().squeeze(), targets_vali.detach().numpy().squeeze())[0,1]\n",
    "            \n",
    "            # remember best acc and save best model\n",
    "            is_best = acc_vali[epoch] >= best_acc\n",
    "            best_acc = max(acc_vali[epoch], best_acc)\n",
    "            save_checkpoint({'epoch': epoch + 1,\n",
    "                            #'arch': args.arch,\n",
    "                            'state_dict': net.state_dict(),\n",
    "                            'best_acc': best_acc,\n",
    "                            'loss': loss_v.item(),\n",
    "                            'R-corrcoef': r_vali,\n",
    "                            'optimizer' : optimizer.state_dict(),\n",
    "                            }, is_best, './Nets/'+zero_filled_number+'_model_best.pth.tar')\n",
    "\n",
    "            if is_best:                \n",
    "                inputs_vali_sur = torch.autograd.Variable(torch.Tensor(X_vali_scaled_sur.astype(np.float32)).float())\n",
    "                targets_vali_sur = torch.autograd.Variable(torch.Tensor(y_vali_class).long())\n",
    "                out_vali_sur = net(inputs_vali_sur)\n",
    "                loss_v_sur = criterion(out_vali_sur, targets_vali_sur.squeeze())\n",
    "                _, predicted_sur = torch.max(out_vali_sur.data, 1)\n",
    "                \n",
    "                # Calcula acurácia\n",
    "                error_count_sur = y_vali_class.size - np.count_nonzero((targets_vali_sur.squeeze() == predicted_sur) .numpy())\n",
    "                acc_vali_sur = 100 * torch.sum(targets_vali_sur.squeeze() == predicted_sur) / y_vali_class.size\n",
    "\n",
    "                r_vali_sur = np.corrcoef(predicted_sur.detach().numpy().squeeze(), targets_vali_sur.detach().numpy().squeeze())[0,1]\n",
    "                \n",
    "                # Confusion matrix\n",
    "                C_vali = confusion_matrix(targets_vali,predicted, labels=[0, 1, 2, 3])\n",
    "                C_perc_vali = C_vali/np.sum(C_vali, axis=1, keepdims=True)*100\n",
    "                \n",
    "#             print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))\n",
    "#             print('   Validation Loss: %.4f' % (loss_v.item()))\n",
    "#             print('   Errors: %d; Accuracy: %d%%' % (error_count, acc_vali[epoch]))\n",
    "#             print('   R-corrcoef: %s' % (str(r_vali)))\n",
    "\n",
    "    # Load best model\n",
    "    checkpoint = torch.load('./Nets/'+zero_filled_number+'_model_best.pth.tar')\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    # Teste\n",
    "    # Avaliando a acurácia do modelo utilizando os dados de teste transformados\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(X_test_scaled.astype(np.float32)).float())\n",
    "    targets = torch.autograd.Variable(torch.Tensor(y_test_class).long())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = net(inputs)\n",
    "    loss = criterion(out, targets.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(out.data, 1)       \n",
    "\n",
    "    error_count = y_test_class.size - np.count_nonzero((targets.squeeze() == predicted) .numpy())\n",
    "    acc = 100 * torch.sum(targets.squeeze() == predicted) /  y_test_class.size\n",
    "    r = np.corrcoef(predicted.detach().numpy().squeeze(), targets.detach().numpy().squeeze())[0,1]\n",
    "\n",
    "#     print('Errors: %d; Accuracy: %d%%' % (error_count, acc))\n",
    "#     print('Teste Loss: %.4f' % (loss.item()))\n",
    "#     print('R-corrcoef: %s' % (str(r)))\n",
    "\n",
    "    # Confusion matrix\n",
    "    C = confusion_matrix(targets,predicted, labels=[0, 1, 2, 3])\n",
    "    C_perc = C/np.sum(C, axis=1, keepdims=True)*100\n",
    "\n",
    "    net_info = {\n",
    "            \"Net\": [zero_filled_number],\n",
    "            \"random_state\": [random_state],\n",
    "            \"num_layers\": [num_layers],\n",
    "            \"layers_size\": [layers_size],\n",
    "            \"net_param\": [net.parameters],\n",
    "            \"criterion\": ['CrossEntropyLoss'],\n",
    "            \"learning_rate\": [learning_rate],\n",
    "            \"optimizer\": ['Adam'],\n",
    "            \"epochs\": [epochs],\n",
    "            \"vali_best_epoch\": [checkpoint['epoch']],\n",
    "            \"vali_best_acc\": [checkpoint['best_acc']],\n",
    "            \"vali_best_loss\": [checkpoint['loss']],\n",
    "            \"vali_best_R\": [checkpoint['R-corrcoef']],\n",
    "            \"vali_sur_acc\": [acc_vali_sur.item()],\n",
    "            \"vali_sur_loss\": [loss_v_sur.item()],\n",
    "            \"vali_sur_R\": [r_vali_sur],\n",
    "            \"vali_c_matrix\": [C_vali],\n",
    "            \"vali_c_matrix_perc\": [C_perc_vali],\n",
    "            \"test_acc\": [acc.item()],\n",
    "            \"test_loss\": [loss.item()],\n",
    "            \"test_R\": [r],\n",
    "            \"test_predicted\": [predicted.numpy()],\n",
    "            \"test_c_matrix\": [C],\n",
    "            \"test_c_matrix_perc\": [C_perc]\n",
    "            }\n",
    "\n",
    "    tags = [\"Net\",\"random_state\",\"num_layers\",\"layers_size\",\"net_param\",\n",
    "            \"criterion\",\"learning_rate\",\"optimizer\",\"epochs\",\n",
    "            \"vali_best_epoch\",\"vali_best_acc\",\"vali_best_loss\",\"vali_best_R\",\n",
    "            \"vali_sur_acc\",\"vali_sur_loss\",\"vali_sur_R\",\"vali_c_matrix\", \n",
    "            \"vali_c_matrix_perc\", \"test_acc\",\"test_loss\",\"test_R\",\n",
    "            \"test_predicted\",\"test_c_matrix\",\"test_c_matrix_perc\"]\n",
    "    df_nets = df_nets.append(pd.DataFrame(net_info, columns = tags), ignore_index=True)\n",
    "\n",
    "    # Add suffix to identify saved info\n",
    "    df_nets.to_excel ('classification_nets_v2.xlsx', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Já incorporar os critérios para salvar as redes?\n",
    "# Matriz de confusão validação apenas 15 dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zika",
   "language": "python",
   "name": "zika"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
