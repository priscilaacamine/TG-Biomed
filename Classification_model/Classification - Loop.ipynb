{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T14:55:37.977806Z",
     "start_time": "2020-12-08T14:55:37.958883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import mode\n",
    "from pickle import dump, load\n",
    "import os\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T14:55:38.322615Z",
     "start_time": "2020-12-08T14:55:38.308653Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_size, layers_size[0])])\n",
    "        for i in range(0, self.num_layers-2):\n",
    "            self.linears.extend([nn.Linear(layers_size[i], layers_size[i+1])])              \n",
    "        self.linears.append(nn.Linear(layers_size[-1], output_size))\n",
    "\n",
    "# Última camada sem função de ativação --> crossentropy já aplica softmax\n",
    "# ReLU em intermediárias\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[0:-1]:                   \n",
    "            x = F.relu(layer(x))\n",
    "        x = (self.linears[-1](x))\n",
    "        return(x)\n",
    "\n",
    "# # Aplicando função de ativação na última camada tbm\n",
    "# ### Tentar mudar pra sigmoide se deixar a normalização de 0,1\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.linears:                   \n",
    "#             x = torch.sigmoid(layer(x))\n",
    "# #             x = F.relu(layer(x))\n",
    "#         return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T14:55:38.714574Z",
     "start_time": "2020-12-08T14:55:38.702908Z"
    }
   },
   "outputs": [],
   "source": [
    "def nonRepeatedRandomInt(low, upper, N):\n",
    "        import numpy as np\n",
    "        import random\n",
    "        \n",
    "        numbers = np.arange(low, upper, 1)\n",
    "        random.shuffle(numbers)\n",
    "        shuffleNumbers = np.array(numbers)[0:int(N)]\n",
    "                \n",
    "        return shuffleNumbers\n",
    "\n",
    "def createSurrogate(X):\n",
    "    Xsur  = np.zeros_like(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        Xsur[:,i] = X[nonRepeatedRandomInt(0, X.shape[0], X.shape[0]),i]\n",
    "    return Xsur\n",
    "\n",
    "def save_checkpoint(state, is_best, filename):\n",
    "    if is_best:\n",
    "        torch.save(state, filename)\n",
    "#         print('*****Saved epoch: %d *****' % (state['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T14:55:39.537038Z",
     "start_time": "2020-12-08T14:55:39.522810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subject\n",
       "1       4\n",
       "2      13\n",
       "3       9\n",
       "4       6\n",
       "6       1\n",
       "7       4\n",
       "8       9\n",
       "10     12\n",
       "11      1\n",
       "12      1\n",
       "13      6\n",
       "14      4\n",
       "15      5\n",
       "16      6\n",
       "17      5\n",
       "18     14\n",
       "19     15\n",
       "20     15\n",
       "21      1\n",
       "22      7\n",
       "23      6\n",
       "24      8\n",
       "25      1\n",
       "26      2\n",
       "27      3\n",
       "28      5\n",
       "29      5\n",
       "30      2\n",
       "31      1\n",
       "32      6\n",
       "33      9\n",
       "34      8\n",
       "35     12\n",
       "36      1\n",
       "37      2\n",
       "38      1\n",
       "39      4\n",
       "40      5\n",
       "41      5\n",
       "42      6\n",
       "43      4\n",
       "44      2\n",
       "45      1\n",
       "46      4\n",
       "47      5\n",
       "48      5\n",
       "49      7\n",
       "50      2\n",
       "51      9\n",
       "52      5\n",
       "53      6\n",
       "54      5\n",
       "56      3\n",
       "57      1\n",
       "61      6\n",
       "62      9\n",
       "63      7\n",
       "64      2\n",
       "65      2\n",
       "66      4\n",
       "67      1\n",
       "68     13\n",
       "69      6\n",
       "70      8\n",
       "71      3\n",
       "72      4\n",
       "73      4\n",
       "74      3\n",
       "75      3\n",
       "76      4\n",
       "77       \n",
       "78      2\n",
       "79      5\n",
       "80     17\n",
       "81      3\n",
       "82      2\n",
       "83      6\n",
       "84      4\n",
       "85      3\n",
       "87      5\n",
       "88     43\n",
       "89     11\n",
       "90      9\n",
       "91      2\n",
       "92      4\n",
       "93      2\n",
       "94      4\n",
       "95      2\n",
       "96      3\n",
       "97      8\n",
       "98      7\n",
       "99      4\n",
       "100     4\n",
       "101     8\n",
       "102     4\n",
       "104     1\n",
       "105    13\n",
       "106     2\n",
       "107     3\n",
       "108     1\n",
       "109     1\n",
       "110     6\n",
       "111     5\n",
       "112     3\n",
       "113    23\n",
       "114     3\n",
       "115     6\n",
       "116     1\n",
       "117    12\n",
       "118     6\n",
       "119     1\n",
       "120     5\n",
       "121     3\n",
       "124     6\n",
       "125     3\n",
       "126     4\n",
       "127     8\n",
       "128     7\n",
       "129     7\n",
       "130     1\n",
       "131     2\n",
       "132    23\n",
       "133     2\n",
       "135     6\n",
       "136     2\n",
       "137     8\n",
       "138     3\n",
       "139    12\n",
       "140     5\n",
       "141     5\n",
       "142     8\n",
       "143     4\n",
       "144     6\n",
       "145     5\n",
       "146     3\n",
       "147     4\n",
       "148     6\n",
       "149     4\n",
       "150    14\n",
       "151    73\n",
       "152    12\n",
       "153     2\n",
       "154     5\n",
       "155     3\n",
       "156     4\n",
       "157     4\n",
       "158     4\n",
       "159     4\n",
       "160     8\n",
       "161     5\n",
       "162     4\n",
       "163     1\n",
       "Name: IPAQ_4b, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['IPAQ_4b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T14:55:40.300316Z",
     "start_time": "2020-12-08T14:55:40.267670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['Age', 'Gender', 'Height', 'Weight', 'BMI', 'FootLen', 'Ystudy', 'Illness', 'Nmedication', 'Ortho-Prosthesis', 'Disability', 'Falls12m', 'FES_1', 'FES_2', 'FES_3', 'FES_4', 'FES_5', 'FES_6', 'FES_7', 'FES_T', 'FES_S', 'IPAQ_1a', 'IPAQ_1b', 'IPAQ_2a', 'IPAQ_2b', 'IPAQ_3a', 'IPAQ_3b', 'IPAQ_4a', 'IPAQ_4b', 'IPAQ_S', 'TMT_timeA', 'TMT_errorsA', 'TMT_timeB', 'TMT_errorsB', 'Best_1', 'Best_2', 'Best_3l', 'Best_3r', 'Best_4', 'Best_5', 'Best_6l', 'Best_6r', 'Best_7', 'Best_8', 'Best_9', 'Best_10', 'Best_11', 'Best_12', 'Best_13', 'Best_14', 'Best_T', 'AngiotensinIIreceptorantagonist', 'HMGCoAreductaseinhibitor', 'Hypercholesterolemia_total', 'Hypertension_total', 'Normal Shoes_total', 'Sandal_FlipFlop_total', 'Dental_total', 'Corrective_lens', 'Hearing_dis_ortho_total', 'Ones']\n",
      "[0.22 0.28 0.36]\n",
      "(106, 1)\n",
      "(46, 1)\n"
     ]
    }
   ],
   "source": [
    "## Salva dado de treino/vali e dado de teste\n",
    "\n",
    "# Read clean data (features <10% excluded) = 25 features\n",
    "data = pd.read_pickle(\"data_closed_foam_adjusted_cleanBDS\")\n",
    "pf50 = np.round(data[\"PF50\"].values, 2)\n",
    "\n",
    "# Usar todas as 24 características + ones\n",
    "features = data.copy()\n",
    "features.drop(\"PF50\", axis=1, inplace=True)\n",
    "\n",
    "features_names = features.columns\n",
    "print('Features: ',list(features_names))\n",
    "X = features.values.astype('float')\n",
    "y = pf50.reshape(len(pf50), 1)\n",
    "\n",
    "# Separando o dataset em treino/vali e teste (treino/vali 70%, teste 30%)\n",
    "X_train_vali, X_test, y_train_vali, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dividehn5 vcnky, em classes de acordo com a os quartis do dado de treino/vali\n",
    "quart = np.quantile(y_train_vali,[0.25, 0.5, 0.75])\n",
    "print(quart)\n",
    "y_train_vali_class = np.digitize(y_train_vali, quart)\n",
    "y_test_class = np.digitize(y_test, quart)\n",
    "print((y_train_vali_class).shape)\n",
    "print((y_test_class).shape)\n",
    "\n",
    "# Salvando os dados em datasets separados\n",
    "\n",
    "np.save('X_testBDS.npy', X_test)\n",
    "np.save('y_testBDS.npy', y_test)\n",
    "np.save('y_test_classBDS.npy', y_test_class)\n",
    "np.save('X_train_valiBDS.npy', X_train_vali)\n",
    "np.save('y_train_valiBDS.npy', y_train_vali)\n",
    "np.save('y_train_vali_classBDS.npy', y_train_vali_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13   0.175  0.2125]\n",
      "(144, 1)\n",
      "(63, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train_vali = np.load('y_train_vali.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Divide em classes de acordo com a os quartis do dado de treino/vali\n",
    "quart = np.quantile(y_train_vali,[0.25, 0.5, 0.75])\n",
    "print(quart)\n",
    "y_train_vali_class = np.digitize(y_train_vali, quart)\n",
    "y_test_class = np.digitize(y_test, quart)\n",
    "print((y_train_vali_class).shape)\n",
    "print((y_test_class).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Net, random_state, num_layers, layers_size, net_param, criterion, learning_rate, optimizer, epochs, vali_best_epoch, vali_best_acc, vali_best_loss, vali_best_R, vali_sur_acc, vali_sur_loss, vali_sur_R, vali_c_matrix, vali_c_matrix_perc, test_acc, test_loss, test_R, test_predicted, test_c_matrix, test_c_matrix_perc]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Create empty xlsx with header\n",
    "# tags = [\"Net\",\"random_state\",\"num_layers\",\"layers_size\",\"net_param\",\n",
    "#         \"criterion\",\"learning_rate\",\"optimizer\",\"epochs\",\n",
    "#         \"vali_best_epoch\",\"vali_best_acc\",\"vali_best_loss\",\"vali_best_R\",\n",
    "#         \"vali_sur_acc\",\"vali_sur_loss\",\"vali_sur_R\",\"vali_c_matrix\", \n",
    "#         \"vali_c_matrix_perc\", \"test_acc\",\"test_loss\",\"test_R\",\n",
    "#         \"test_predicted\",\"test_c_matrix\",\"test_c_matrix_perc\"]\n",
    "# df_nets = pd.DataFrame(columns = tags)\n",
    "# print(df_nets)\n",
    "# df_nets.to_excel ('classification_nets_empty.xlsx', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda3\\envs\\zika\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# loop de redes\n",
    "Nnets = 5\n",
    "# Load initial xlsx\n",
    "df_nets = pd.read_excel('classification_nets_empty.xlsx', index_col=None, header=0)\n",
    "\n",
    "# Load data\n",
    "X_train_vali = np.load('X_train_vali.npy')\n",
    "y_train_vali_class = np.load('y_train_vali_class.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test_class = np.load('y_test_class.npy')\n",
    "\n",
    "if os.path.exists('Nets') == False: \n",
    "    os.makedirs('Nets')\n",
    "\n",
    "for i in range(Nnets):\n",
    "    number_str = str(i)\n",
    "    zero_filled_number = number_str.zfill(5)\n",
    "    \n",
    "    # Separando o treino da vali (treino 80%, validação 20%)\n",
    "    # Deixar o high variável?\n",
    "    random_state = np.random.randint(0, Nnets*10, 1)[0]\n",
    "#     print('Random State: %d' % (random_state))\n",
    "    X_train, X_vali, y_train_class, y_vali_class = train_test_split(X_train_vali, y_train_vali_class, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    # Escalamento e Transformação dos dados\n",
    "    scaler_x = RobustScaler(with_centering=True)\n",
    "    X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "    X_vali_scaled = scaler_x.transform(X_vali)\n",
    "    X_test_scaled = scaler_x.transform(X_test)\n",
    "    # save the scaler\n",
    "    dump(scaler_x, open('./Nets/'+zero_filled_number+'_scaler_x.pkl', 'wb'))\n",
    "    \n",
    "    # Create Surrogate ## Para validação ou para teste?\n",
    "    X_vali_scaled_sur = createSurrogate(X_vali_scaled)\n",
    "    \n",
    "    # Parâmetros da rede\n",
    "    torch.manual_seed(1234)\n",
    "    num_layers = 4\n",
    "#     print('Number of layers: %d' % (num_layers))\n",
    "    layer_init_size = np.random.randint(low=1, high=24)\n",
    "#     print('Inicial size layer: %d' % (layer_init_size))\n",
    "    layer_2_size = np.random.randint(low=1, high=24)\n",
    "    layers_size = [layer_init_size, layer_2_size, 4]\n",
    "#     print('Layers sizes:', list(layers_size))\n",
    "    net = Net(input_size=X_train.shape[1], num_layers=num_layers, layers_size=layers_size , output_size=4)\n",
    "    \n",
    "    # Choose optmizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = np.random.randint(low=1, high=100)/1000\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "    \n",
    "    # Treinamento \n",
    "    epochs = 50000\n",
    "    loss_train = np.zeros(epochs)\n",
    "    loss_vali = np.zeros(epochs)\n",
    "    acc_vali = np.zeros(epochs)\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        inputs = torch.autograd.Variable(torch.Tensor(X_train_scaled.astype(np.float32)).float())\n",
    "        targets = torch.autograd.Variable(torch.Tensor(y_train_class).long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = net(inputs)\n",
    "        loss = criterion(out, targets.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "\n",
    "        # Validação\n",
    "        if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "            inputs_vali = torch.autograd.Variable(torch.Tensor(X_vali_scaled.astype(np.float32)).float())\n",
    "            targets_vali = torch.autograd.Variable(torch.Tensor(y_vali_class).long())\n",
    "            out_vali = net(inputs_vali)\n",
    "            loss_v = criterion(out_vali, targets_vali.squeeze())\n",
    "            loss_vali[epoch] = loss_v.item()\n",
    "            _, predicted = torch.max(out_vali.data, 1)\n",
    "\n",
    "            # Calcula acurácia\n",
    "            error_count = y_vali_class.size - np.count_nonzero((targets_vali.squeeze() == predicted) .numpy())\n",
    "            acc_vali[epoch] = 100 * torch.sum(targets_vali.squeeze() == predicted) / y_vali_class.size\n",
    "            \n",
    "            r_vali = np.corrcoef(predicted.detach().numpy().squeeze(), targets_vali.detach().numpy().squeeze())[0,1]\n",
    "            \n",
    "            # remember best acc and save best model\n",
    "            is_best = acc_vali[epoch] >= best_acc\n",
    "            best_acc = max(acc_vali[epoch], best_acc)\n",
    "            save_checkpoint({'epoch': epoch + 1,\n",
    "                            #'arch': args.arch,\n",
    "                            'state_dict': net.state_dict(),\n",
    "                            'best_acc': best_acc,\n",
    "                            'loss': loss_v.item(),\n",
    "                            'R-corrcoef': r_vali,\n",
    "                            'optimizer' : optimizer.state_dict(),\n",
    "                            }, is_best, './Nets/'+zero_filled_number+'_model_best.pth.tar')\n",
    "\n",
    "            if is_best:                \n",
    "                inputs_vali_sur = torch.autograd.Variable(torch.Tensor(X_vali_scaled_sur.astype(np.float32)).float())\n",
    "                targets_vali_sur = torch.autograd.Variable(torch.Tensor(y_vali_class).long())\n",
    "                out_vali_sur = net(inputs_vali_sur)\n",
    "                loss_v_sur = criterion(out_vali_sur, targets_vali_sur.squeeze())\n",
    "                _, predicted_sur = torch.max(out_vali_sur.data, 1)\n",
    "                \n",
    "                # Calcula acurácia\n",
    "                error_count_sur = y_vali_class.size - np.count_nonzero((targets_vali_sur.squeeze() == predicted_sur) .numpy())\n",
    "                acc_vali_sur = 100 * torch.sum(targets_vali_sur.squeeze() == predicted_sur) / y_vali_class.size\n",
    "\n",
    "                r_vali_sur = np.corrcoef(predicted_sur.detach().numpy().squeeze(), targets_vali_sur.detach().numpy().squeeze())[0,1]\n",
    "                \n",
    "                # Confusion matrix\n",
    "                C_vali = confusion_matrix(targets_vali,predicted, labels=[0, 1, 2, 3])\n",
    "                C_perc_vali = C_vali/np.sum(C_vali, axis=1, keepdims=True)*100\n",
    "                \n",
    "#             print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))\n",
    "#             print('   Validation Loss: %.4f' % (loss_v.item()))\n",
    "#             print('   Errors: %d; Accuracy: %d%%' % (error_count, acc_vali[epoch]))\n",
    "#             print('   R-corrcoef: %s' % (str(r_vali)))\n",
    "\n",
    "    # Load best model\n",
    "    checkpoint = torch.load('./Nets/'+zero_filled_number+'_model_best.pth.tar')\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    # Teste\n",
    "    # Avaliando a acurácia do modelo utilizando os dados de teste transformados\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(X_test_scaled.astype(np.float32)).float())\n",
    "    targets = torch.autograd.Variable(torch.Tensor(y_test_class).long())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = net(inputs)\n",
    "    loss = criterion(out, targets.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(out.data, 1)       \n",
    "\n",
    "    error_count = y_test_class.size - np.count_nonzero((targets.squeeze() == predicted) .numpy())\n",
    "    acc = 100 * torch.sum(targets.squeeze() == predicted) /  y_test_class.size\n",
    "    r = np.corrcoef(predicted.detach().numpy().squeeze(), targets.detach().numpy().squeeze())[0,1]\n",
    "\n",
    "#     print('Errors: %d; Accuracy: %d%%' % (error_count, acc))\n",
    "#     print('Teste Loss: %.4f' % (loss.item()))\n",
    "#     print('R-corrcoef: %s' % (str(r)))\n",
    "\n",
    "    # Confusion matrix\n",
    "    C = confusion_matrix(targets,predicted, labels=[0, 1, 2, 3])\n",
    "    C_perc = C/np.sum(C, axis=1, keepdims=True)*100\n",
    "\n",
    "    net_info = {\n",
    "            \"Net\": [zero_filled_number],\n",
    "            \"random_state\": [random_state],\n",
    "            \"num_layers\": [num_layers],\n",
    "            \"layers_size\": [layers_size],\n",
    "            \"net_param\": [net.parameters],\n",
    "            \"criterion\": ['CrossEntropyLoss'],\n",
    "            \"learning_rate\": [learning_rate],\n",
    "            \"optimizer\": ['Adam'],\n",
    "            \"epochs\": [epochs],\n",
    "            \"vali_best_epoch\": [checkpoint['epoch']],\n",
    "            \"vali_best_acc\": [checkpoint['best_acc']],\n",
    "            \"vali_best_loss\": [checkpoint['loss']],\n",
    "            \"vali_best_R\": [checkpoint['R-corrcoef']],\n",
    "            \"vali_sur_acc\": [acc_vali_sur.item()],\n",
    "            \"vali_sur_loss\": [loss_v_sur.item()],\n",
    "            \"vali_sur_R\": [r_vali_sur],\n",
    "            \"vali_c_matrix\": [C_vali],\n",
    "            \"vali_c_matrix_perc\": [C_perc_vali],\n",
    "            \"test_acc\": [acc.item()],\n",
    "            \"test_loss\": [loss.item()],\n",
    "            \"test_R\": [r],\n",
    "            \"test_predicted\": [predicted.numpy()],\n",
    "            \"test_c_matrix\": [C],\n",
    "            \"test_c_matrix_perc\": [C_perc]\n",
    "            }\n",
    "\n",
    "    tags = [\"Net\",\"random_state\",\"num_layers\",\"layers_size\",\"net_param\",\n",
    "            \"criterion\",\"learning_rate\",\"optimizer\",\"epochs\",\n",
    "            \"vali_best_epoch\",\"vali_best_acc\",\"vali_best_loss\",\"vali_best_R\",\n",
    "            \"vali_sur_acc\",\"vali_sur_loss\",\"vali_sur_R\",\"vali_c_matrix\", \n",
    "            \"vali_c_matrix_perc\", \"test_acc\",\"test_loss\",\"test_R\",\n",
    "            \"test_predicted\",\"test_c_matrix\",\"test_c_matrix_perc\"]\n",
    "    df_nets = df_nets.append(pd.DataFrame(net_info, columns = tags), ignore_index=True)\n",
    "\n",
    "    # Add suffix to identify saved info\n",
    "    df_nets.to_excel ('classification_nets_v2.xlsx', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Já incorporar os critérios para salvar as redes?\n",
    "# Matriz de confusão validação apenas 15 dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegar 20% validação"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
