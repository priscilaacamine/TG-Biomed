{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import mode\n",
    "from pickle import dump, load\n",
    "import os\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_size, layers_size[0])])\n",
    "        for i in range(0, self.num_layers-2):\n",
    "            self.linears.extend([nn.Linear(layers_size[i], layers_size[i+1])])              \n",
    "        self.linears.append(nn.Linear(layers_size[-1], output_size))\n",
    "\n",
    "# Última camada sem função de ativação --> crossentropy já aplica softmax\n",
    "# ReLU em intermediárias\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[0:-1]:                   \n",
    "            x = F.relu(layer(x))\n",
    "        x = (self.linears[-1](x))\n",
    "        return(x)\n",
    "\n",
    "# # Aplicando função de ativação na última camada tbm\n",
    "# ### Tentar mudar pra sigmoide se deixar a normalização de 0,1\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.linears:                   \n",
    "#             x = torch.sigmoid(layer(x))\n",
    "# #             x = F.relu(layer(x))\n",
    "#         return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Net</th>\n",
       "      <th>random_state</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>layers_size</th>\n",
       "      <th>net_param</th>\n",
       "      <th>criterion</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>epochs</th>\n",
       "      <th>vali_best_epoch</th>\n",
       "      <th>...</th>\n",
       "      <th>vali_sur_loss</th>\n",
       "      <th>vali_sur_R</th>\n",
       "      <th>vali_c_matrix</th>\n",
       "      <th>vali_c_matrix_perc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_R</th>\n",
       "      <th>test_predicted</th>\n",
       "      <th>test_c_matrix</th>\n",
       "      <th>test_c_matrix_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>[14, 23, 4]</td>\n",
       "      <td>&lt;bound method Module.parameters of Net(\\n  (li...</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>0.017</td>\n",
       "      <td>Adam</td>\n",
       "      <td>50000</td>\n",
       "      <td>23700</td>\n",
       "      <td>...</td>\n",
       "      <td>48.692993</td>\n",
       "      <td>-0.212718</td>\n",
       "      <td>[[4 1 1 0]\\n [0 2 1 0]\\n [0 0 3 0]\\n [1 1 0 1]]</td>\n",
       "      <td>[[ 66.66666667  16.66666667  16.66666667   0. ...</td>\n",
       "      <td>22</td>\n",
       "      <td>44.922821</td>\n",
       "      <td>0.196643</td>\n",
       "      <td>[2 1 3 1 2 1 2 1 1 1 1 3 2 1 0 2 0 2 2 3 0 3 3...</td>\n",
       "      <td>[[6 9 3 5]\\n [3 3 7 2]\\n [1 6 3 3]\\n [0 4 6 2]]</td>\n",
       "      <td>[[26.08695652 39.13043478 13.04347826 21.73913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>[11, 12, 4]</td>\n",
       "      <td>&lt;bound method Module.parameters of Net(\\n  (li...</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>0.034</td>\n",
       "      <td>Adam</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.388716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[0 3 0 0]\\n [0 5 0 0]\\n [0 2 0 0]\\n [0 5 0 0]]</td>\n",
       "      <td>[[  0. 100.   0.   0.]\\n [  0. 100.   0.   0.]...</td>\n",
       "      <td>23</td>\n",
       "      <td>1.427481</td>\n",
       "      <td>0.124289</td>\n",
       "      <td>[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...</td>\n",
       "      <td>[[ 0 23  0  0]\\n [ 0 14  0  1]\\n [ 0 13  0  0]...</td>\n",
       "      <td>[[  0.         100.           0.           0. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>[6, 8, 4]</td>\n",
       "      <td>&lt;bound method Module.parameters of Net(\\n  (li...</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Adam</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[6 0 0 0]\\n [1 0 0 0]\\n [5 0 0 0]\\n [3 0 0 0]]</td>\n",
       "      <td>[[100.   0.   0.   0.]\\n [100.   0.   0.   0.]...</td>\n",
       "      <td>36</td>\n",
       "      <td>1.372726</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>[0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0...</td>\n",
       "      <td>[[22  0  1  0]\\n [14  0  1  0]\\n [12  0  1  0]...</td>\n",
       "      <td>[[95.65217391  0.          4.34782609  0.     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>[22, 11, 4]</td>\n",
       "      <td>&lt;bound method Module.parameters of Net(\\n  (li...</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>0.078</td>\n",
       "      <td>Adam</td>\n",
       "      <td>50000</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>56.450802</td>\n",
       "      <td>0.105561</td>\n",
       "      <td>[[0 1 1 1]\\n [0 2 0 3]\\n [0 2 1 2]\\n [0 0 0 2]]</td>\n",
       "      <td>[[  0.          33.33333333  33.33333333  33.3...</td>\n",
       "      <td>25</td>\n",
       "      <td>30.808821</td>\n",
       "      <td>-0.015465</td>\n",
       "      <td>[1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 3 1 1 1 3 2 1 1...</td>\n",
       "      <td>[[ 0 14  6  3]\\n [ 0 13  0  2]\\n [ 0 11  1  1]...</td>\n",
       "      <td>[[ 0.         60.86956522 26.08695652 13.04347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>[8, 17, 4]</td>\n",
       "      <td>&lt;bound method Module.parameters of Net(\\n  (li...</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>0.062</td>\n",
       "      <td>Adam</td>\n",
       "      <td>50000</td>\n",
       "      <td>2500</td>\n",
       "      <td>...</td>\n",
       "      <td>149.926834</td>\n",
       "      <td>-0.018179</td>\n",
       "      <td>[[2 0 0 4]\\n [0 1 0 2]\\n [0 0 0 3]\\n [0 0 0 3]]</td>\n",
       "      <td>[[ 33.33333333   0.           0.          66.6...</td>\n",
       "      <td>22</td>\n",
       "      <td>13.291863</td>\n",
       "      <td>0.072834</td>\n",
       "      <td>[3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 0...</td>\n",
       "      <td>[[ 4  0  0 19]\\n [ 2  0  0 13]\\n [ 1  2  0 10]...</td>\n",
       "      <td>[[17.39130435  0.          0.         82.60869...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Net  random_state  num_layers  layers_size  \\\n",
       "0    0            38           4  [14, 23, 4]   \n",
       "1    1            44           4  [11, 12, 4]   \n",
       "2    2             6           4    [6, 8, 4]   \n",
       "3    3            47           4  [22, 11, 4]   \n",
       "4    4            38           4   [8, 17, 4]   \n",
       "\n",
       "                                           net_param         criterion  \\\n",
       "0  <bound method Module.parameters of Net(\\n  (li...  CrossEntropyLoss   \n",
       "1  <bound method Module.parameters of Net(\\n  (li...  CrossEntropyLoss   \n",
       "2  <bound method Module.parameters of Net(\\n  (li...  CrossEntropyLoss   \n",
       "3  <bound method Module.parameters of Net(\\n  (li...  CrossEntropyLoss   \n",
       "4  <bound method Module.parameters of Net(\\n  (li...  CrossEntropyLoss   \n",
       "\n",
       "   learning_rate optimizer  epochs  vali_best_epoch  ...  vali_sur_loss  \\\n",
       "0          0.017      Adam   50000            23700  ...      48.692993   \n",
       "1          0.034      Adam   50000            50000  ...       1.388716   \n",
       "2          0.009      Adam   50000                1  ...       1.330297   \n",
       "3          0.078      Adam   50000              600  ...      56.450802   \n",
       "4          0.062      Adam   50000             2500  ...     149.926834   \n",
       "\n",
       "   vali_sur_R                                    vali_c_matrix  \\\n",
       "0   -0.212718  [[4 1 1 0]\\n [0 2 1 0]\\n [0 0 3 0]\\n [1 1 0 1]]   \n",
       "1         NaN  [[0 3 0 0]\\n [0 5 0 0]\\n [0 2 0 0]\\n [0 5 0 0]]   \n",
       "2         NaN  [[6 0 0 0]\\n [1 0 0 0]\\n [5 0 0 0]\\n [3 0 0 0]]   \n",
       "3    0.105561  [[0 1 1 1]\\n [0 2 0 3]\\n [0 2 1 2]\\n [0 0 0 2]]   \n",
       "4   -0.018179  [[2 0 0 4]\\n [0 1 0 2]\\n [0 0 0 3]\\n [0 0 0 3]]   \n",
       "\n",
       "                                  vali_c_matrix_perc  test_acc  test_loss  \\\n",
       "0  [[ 66.66666667  16.66666667  16.66666667   0. ...        22  44.922821   \n",
       "1  [[  0. 100.   0.   0.]\\n [  0. 100.   0.   0.]...        23   1.427481   \n",
       "2  [[100.   0.   0.   0.]\\n [100.   0.   0.   0.]...        36   1.372726   \n",
       "3  [[  0.          33.33333333  33.33333333  33.3...        25  30.808821   \n",
       "4  [[ 33.33333333   0.           0.          66.6...        22  13.291863   \n",
       "\n",
       "     test_R                                     test_predicted  \\\n",
       "0  0.196643  [2 1 3 1 2 1 2 1 1 1 1 3 2 1 0 2 0 2 2 3 0 3 3...   \n",
       "1  0.124289  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...   \n",
       "2  0.222700  [0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
       "3 -0.015465  [1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 3 1 1 1 3 2 1 1...   \n",
       "4  0.072834  [3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 0...   \n",
       "\n",
       "                                       test_c_matrix  \\\n",
       "0    [[6 9 3 5]\\n [3 3 7 2]\\n [1 6 3 3]\\n [0 4 6 2]]   \n",
       "1  [[ 0 23  0  0]\\n [ 0 14  0  1]\\n [ 0 13  0  0]...   \n",
       "2  [[22  0  1  0]\\n [14  0  1  0]\\n [12  0  1  0]...   \n",
       "3  [[ 0 14  6  3]\\n [ 0 13  0  2]\\n [ 0 11  1  1]...   \n",
       "4  [[ 4  0  0 19]\\n [ 2  0  0 13]\\n [ 1  2  0 10]...   \n",
       "\n",
       "                                  test_c_matrix_perc  \n",
       "0  [[26.08695652 39.13043478 13.04347826 21.73913...  \n",
       "1  [[  0.         100.           0.           0. ...  \n",
       "2  [[95.65217391  0.          4.34782609  0.     ...  \n",
       "3  [[ 0.         60.86956522 26.08695652 13.04347...  \n",
       "4  [[17.39130435  0.          0.         82.60869...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read xlsx with nets info\n",
    "df_nets = pd.read_excel('classification_nets_v2.xlsx', index_col=None, header=0)\n",
    "df_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test_class = np.load('y_test_class.npy')\n",
    "\n",
    "all_predicted = np.empty([df_nets.shape[0], y_test_class.shape[0]])\n",
    "\n",
    "for i in range(df_nets.shape[0]):\n",
    "    netX = df_nets.iloc[i]\n",
    "    net_number = netX['Net']\n",
    "    number_str = str(net_number)\n",
    "    zero_filled_number = number_str.zfill(5)\n",
    "    # Transformando o dado de teste\n",
    "    # load the scaler\n",
    "    scaler_x = load(open('./Nets/'+zero_filled_number+'_scaler_x.pkl', 'rb'))\n",
    "    X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "    # Parâmetros da rede\n",
    "    torch.manual_seed(1234)\n",
    "    num_layers = netX['num_layers']\n",
    "    # print('Number of layers: %d' % (num_layers))\n",
    "    layers_size = np.fromstring(netX['layers_size'][1:-1], dtype=int, sep=',')\n",
    "    # print('Layers sizes:', layers_size)\n",
    "\n",
    "    net = Net(input_size=X_test.shape[1], num_layers=num_layers, layers_size=layers_size , output_size=4)\n",
    "\n",
    "    # Optmizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = netX['learning_rate']\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "\n",
    "    # Load best model\n",
    "    checkpoint = torch.load('./Nets/'+zero_filled_number+'_model_best.pth.tar')\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    # Avaliando a acurácia do modelo utilizando os dados de teste transformados\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(X_test_scaled.astype(np.float32)).float())\n",
    "    targets = torch.autograd.Variable(torch.Tensor(y_test_class).long())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = net(inputs)\n",
    "    loss = criterion(out, targets.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(out.data, 1)       \n",
    "\n",
    "    error_count = y_test_class.size - np.count_nonzero((targets.squeeze() == predicted) .numpy())\n",
    "    acc = 100 * torch.sum(targets.squeeze() == predicted) /  y_test_class.size\n",
    "    r = np.corrcoef(predicted.detach().numpy().squeeze(), targets.detach().numpy().squeeze())[0,1]\n",
    "\n",
    "    # print('Errors: %d; Accuracy: %d%%' % (error_count, acc))\n",
    "    # print('Teste Loss: %.4f' % (loss.item()))\n",
    "    # print('R-corrcoef: %s' % (str(r)))\n",
    "\n",
    "    all_predicted[i,:] = predicted.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 63)\n",
      "[[2. 1. 3. 1. 2. 1. 2. 1. 1. 1. 1. 3. 2. 1. 0. 2. 0. 2. 2. 3. 0. 3. 3. 3.\n",
      "  2. 2. 1. 1. 3. 0. 1. 1. 0. 0. 3. 2. 0. 2. 3. 1. 1. 1. 2. 1. 2. 2. 1. 2.\n",
      "  3. 0. 0. 1. 2. 1. 3. 1. 2. 0. 3. 1. 2. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 2.\n",
      "  0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 3. 1. 1. 1. 3. 2. 1. 1. 2.\n",
      "  1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 3. 1. 2. 1. 1. 1. 2. 1.\n",
      "  3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 3. 1. 3. 2. 3.]\n",
      " [3. 3. 3. 0. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 3. 0. 3.\n",
      "  3. 3. 0. 3. 3. 3. 3. 3. 1. 0. 1. 3. 3. 3. 3. 3. 3. 3. 3. 0. 3. 3. 0. 3.\n",
      "  3. 3. 3. 1. 3. 3. 3. 3. 3. 3. 3. 3. 2. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(all_predicted.shape)\n",
    "print(all_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 3. 0. 1. 1. 3. 0. 1. 0. 3.\n",
      "  1. 1. 1. 1. 3. 0. 1. 1. 1. 0. 1. 2. 0. 1. 3. 1. 1. 1. 2. 1. 1. 1. 0. 1.\n",
      "  3. 0. 0. 1. 1. 1. 3. 1. 1. 0. 3. 1. 2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "voteClassOut, count = mode(all_predicted, axis=0)\n",
    "print(voteClassOut)\n",
    "voteClassOut = voteClassOut.reshape(63,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 1 2 1 2 2 2 1 3 1 1 0 2 0 0 0 2 1 3 3 3 2 1 0 0 0 2 2 1 2 1\n",
      "  0 1 0 0 0 0 3 0 2 0 0 3 1 0 0 3 2 1 3 3 1 1 0 2 3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_class.reshape(1, 63))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia 28.57142857142857 %\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(y_test_class, 'ob')\n",
    "plt.plot(voteClassOut, 'xr')\n",
    "plt.show()\n",
    "print('Acurácia', np.sum(y_test_class.squeeze()==voteClassOut.squeeze())/len(y_test_class)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot predicted vs target de uma rede\n",
    "# ninput = np.arange(len(X_test))\n",
    "# plt.figure()\n",
    "# plt.plot(ninput, y_test_class, '-*', label = 'target')\n",
    "# plt.plot(ninput, predicted.numpy(), '-+', label = 'predicted')\n",
    "# plt.title('Dados Normalizados')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zika",
   "language": "python",
   "name": "zika"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
