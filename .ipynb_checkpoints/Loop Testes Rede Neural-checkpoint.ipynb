{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_size, layers_size[0])])\n",
    "        for i in range(0, self.num_layers-2):\n",
    "            self.linears.extend([nn.Linear(layers_size[i], layers_size[i+1])])              \n",
    "        self.linears.append(nn.Linear(layers_size[-1], output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[0:-1]:                   \n",
    "            x = torch.tanh(layer(x))\n",
    "        x = self.linears[-1](x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonRepeatedRandomInt(low, upper, N):\n",
    "        import numpy as np\n",
    "        import random\n",
    "        \n",
    "        numbers = np.arange(low, upper, 1)\n",
    "        random.shuffle(numbers)\n",
    "        shuffleNumbers = np.array(numbers)[0:int(N)]\n",
    "        shuffleNumbers = np.sort(shuffleNumbers)\n",
    "        \n",
    "        return shuffleNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(file, idx, net_param, ncamadas, n_features, features_col, features_name, random_state, data_train, data_train_resample, test_loss, test_cos_sim, net):\n",
    "    import csv\n",
    "    \n",
    "    \n",
    "    headerCSVList= []\n",
    "   \n",
    "    headerCSVList.append('index')\n",
    "    headerCSVList.append('net_param')\n",
    "    headerCSVList.append('ncamadas')\n",
    "    headerCSVList.append('n_features')\n",
    "    headerCSVList.append('features_col')\n",
    "    headerCSVList.append('features_name')\n",
    "    headerCSVList.append('random_state')\n",
    "    headerCSVList.append('data_train')\n",
    "    headerCSVList.append('data_train_resample')\n",
    "    headerCSVList.append('test_loss')\n",
    "    headerCSVList.append('test_cos_sim')\n",
    "    headerCSVList.append('net')\n",
    "    \n",
    "    #---printing file results---------\n",
    "    rowCSVList=[]   \n",
    "    rowCSVList.append(idx)\n",
    "    rowCSVList.append(net_param)\n",
    "    rowCSVList.append(ncamadas)\n",
    "    rowCSVList.append(n_features)\n",
    "    rowCSVList.append(features_col)\n",
    "    rowCSVList.append(features_name)\n",
    "    rowCSVList.append(random_state)\n",
    "    rowCSVList.append(data_train)\n",
    "    rowCSVList.append(data_train_resample)\n",
    "    rowCSVList.append(test_loss)\n",
    "    rowCSVList.append(test_cos_sim)\n",
    "    rowCSVList.append(net)\n",
    "    \n",
    "   \n",
    "    with open(file, 'a') as csvfileWrite:\n",
    "        spamwriter = csv.writer(csvfileWrite, delimiter=',')\n",
    "        if idx==50000:\n",
    "            spamwriter.writerow(headerCSVList)\n",
    "           \n",
    "        spamwriter.writerow(rowCSVList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for teste in range(66601, 100000):\n",
    "    data = pd.read_pickle(\"data_closed_rigid_o\")\n",
    "    pf50 = data[\"PF50_closed_rigid\"].values\n",
    "\n",
    "    # Sortear quais caracteristicas usar\n",
    "    n_features = np.random.randint(low=1, high=26)\n",
    "    #print('Numero de features: %d' % (n_features))\n",
    "    columns = nonRepeatedRandomInt(low=0, upper=183, N=n_features)\n",
    "    #print(columns)\n",
    "\n",
    "    features = data.iloc[:,columns]\n",
    "    features_names = features.columns\n",
    "    X = features.values\n",
    "    y = pf50.reshape(len(pf50), 1)\n",
    "\n",
    "    # Separando o dataset em treino e teste\n",
    "    random_state =  np.random.randint(low=1, high=100)\n",
    "    #print('Random State: %d' % (random_state))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "    # Histograma dos dados de treino\n",
    "    for nbins in range(10,1,-1):\n",
    "        n, bins, patches = plt.hist(y_train, nbins)\n",
    "        if np.min(n)>3:\n",
    "            break\n",
    "\n",
    "    # Indica qual o bin correspondente para cada dado\n",
    "    indices = np.digitize(y_train, bins)\n",
    "\n",
    "    # Arrumar o caso do último valor que não é incluído no último bin\n",
    "    indices[indices>nbins]=nbins\n",
    "    indices = indices.reshape(len(indices),)\n",
    "\n",
    "    # SMOTE - Data Augmentation\n",
    "    data_train = np.hstack([X_train, y_train])\n",
    "    sm = SMOTE(k_neighbors=np.int(n.min()-1), random_state=42)\n",
    "    data_train_res, indices_res = sm.fit_resample(data_train, indices)\n",
    "\n",
    "    # Histograma dos dados de treino resample\n",
    "    nbins = 5\n",
    "    n_res, bins_res, patches_res = plt.hist(data_train_res[:,-1], nbins)\n",
    "\n",
    "    # Declarando o dado resample como dado de treino\n",
    "    X_train = data_train_res[:,0:-1]\n",
    "    y_train = data_train_res[:,-1].reshape(data_train_res.shape[0],1)\n",
    "\n",
    "    # Ajustando o escalamento ao dado de TREINAMENTO (fit) e transformando o dado de treinamento\n",
    "    scaler_x = MinMaxScaler()\n",
    "    X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    num_layers = np.random.randint(low=3, high=10)\n",
    "    #print('Number of layers: %d' % (num_layers))\n",
    "    layer_init_size = np.random.randint(low=1, high=20)\n",
    "    #print('Inicial size layer: %d' % (layer_init_size))\n",
    "    layers_size = np.linspace(layer_init_size, 1, num_layers-1, dtype=int)\n",
    "    #print(layers_size)\n",
    "\n",
    "    net = Net(input_size=X.shape[1], num_layers=num_layers, layers_size=layers_size , output_size=1)\n",
    "\n",
    "    # Choose optmizer and loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = 0.05, momentum = 0.9) # lr = learning rate\n",
    "\n",
    "    # Treinamento utilizando o dado de treino transformado\n",
    "    epochs = 5000\n",
    "    lossr = np.zeros(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        inputs = torch.autograd.Variable(torch.Tensor(X_train_scaled.astype(np.float32)).float())\n",
    "        targets = torch.autograd.Variable(torch.Tensor(y_train_scaled.astype(np.float32)).float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = net(inputs)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossr[epoch] = loss.item()\n",
    "\n",
    "        #if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "            #print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))\n",
    "\n",
    "    torch.save(net.state_dict(), 'F:\\\\Priscila Acamine\\\\TG-Biomed\\\\Nets2_50000\\\\net_'+str(teste))\n",
    "\n",
    "    epoch = np.arange(0, epochs)\n",
    "    # plt.plot(epoch,lossr)\n",
    "    # plt.show()\n",
    "\n",
    "    # Transformando o dado de teste\n",
    "    X_test_scaled = scaler_x.fit_transform(X_test)\n",
    "    y_test_scaled = scaler_y.fit_transform(y_test)\n",
    "\n",
    "    # Avaliando a acurácia do modelo utilizando os dados de teste transformados\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(X_test_scaled.astype(np.float32)).float())\n",
    "    targets = torch.autograd.Variable(torch.Tensor(y_test_scaled.astype(np.float32)).float())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = net(inputs)\n",
    "    loss = criterion(out, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    predicted = out.data\n",
    "\n",
    "#     print('Teste Loss: %.4f' % (loss.item()))\n",
    "\n",
    "    # Implementando a métrica do Cosine Similarity\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cos_sim = cos(targets, predicted)\n",
    "#     print('Teste Cosine Similarity: %.4f' % (cos_sim.item()))\n",
    "\n",
    "    ninput = np.arange(len(X_test))\n",
    "    '''plt.figure()\n",
    "    plt.plot(ninput, y_test_scaled, '-*', label = 'target')\n",
    "    plt.plot(ninput, predicted.numpy(), '-+', label = 'predicted')\n",
    "    plt.title('Dados Normalizados')\n",
    "    plt.legend()\n",
    "    plt.show()'''\n",
    "\n",
    "    # Fazendo a transformação inversa para desnormalizar o dado de teste\n",
    "    predicted_desnorm = scaler_y.inverse_transform(predicted.numpy())\n",
    "    target_desnorm = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "    '''plt.figure()\n",
    "    plt.plot(ninput, target_desnorm, '-*', label = 'target')\n",
    "    plt.plot(ninput, predicted_desnorm, '-+', label = 'predicted')\n",
    "    plt.title('Desnormalizado target_desnorm')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    '''\n",
    "    print_results(file='Loop_Testes_Regressao2.csv', idx=teste, net_param=net.parameters, ncamadas=num_layers, n_features=n_features, features_col=columns, \n",
    "                  features_name=list(features_names), random_state=random_state, data_train=data_train.shape[0], \n",
    "                  data_train_resample=data_train_res.shape[0], test_loss=loss.item(), test_cos_sim=cos_sim.item(), net='net_'+str(teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
